<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 21 Dec 2025 12:06:42 +0000</lastBuildDate><item><title>ST-Imputer: Multivariate Dependency-aware Diffusion Network with Physics Guidance for Spatiotemporal Imputation</title><link>https://doi.org/10.1016/j.inffus.2025.104084</link><guid>10.1016/j.inffus.2025.104084</guid><pubDate>Sat, 20 Dec 2025 23:22:45 +0000</pubDate><dc:creator>Xingyu Zhao</dc:creator><dc:creator>Jianpeng Qi</dc:creator><dc:creator>Bin Lu</dc:creator><dc:creator>Lei Zhou</dc:creator><dc:creator>Lei Cao</dc:creator><dc:creator>Junyu Dong</dc:creator><dc:creator>Yanwei Yu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104084</prism:doi><description>Data preparation is crucial for achieving optimal results in deep learning. Unfortunately, missing values are common when preparing large-scale spatiotemporal databases. Most existing imputation methods primarily focus on exploring the spatiotemporal correlations of single-source data; however, high missing rates in single-source data result in sparse distributions. Furthermore, existing methods typically focus on shallow correlations at a single scale, limiting the ability of imputation models to effectively leverage multi-scale spatial features. To tackle these challenges, we propose a multivariate dependency-aware spatiotemporal imputation model, named ST-Imputer. Specifically, we introduce multi-source context data to provide sufficient correlation features for target data ( i.e ., data that needs imputation), alleviating the issue of insufficient available features caused by high missing rates in single-source data. By applying a multi-variate spatiotemporal dependency extraction module, ST-Imputer captures potential associations between different spatial scales. Subsequently, the noise prediction module utilizes the learned dual-view features to formulate the spatiotemporal transmission module, thereby reducing weight errors caused by excessive noise. Finally, physical constraints are applied to prevent unrealistic predictions. Extensive experiments on three large-scale datasets demonstrate the significant superiority of ST-Imputer, achieving up to a 13.07% improvement in RMSE. The code of our model is available at https://github.com/Lion1a/ST-Imputer .
Published: 2025-12-20T23:22:45+00:00
Venue: Information Fusion
Score: 0.413 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingyu Zhao; Jianpeng Qi; Bin Lu; Lei Zhou; Lei Cao; Junyu Dong; Yanwei Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104084"&gt;10.1016/j.inffus.2025.104084&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.413 (consider)&lt;/p&gt;
&lt;p&gt;Data preparation is crucial for achieving optimal results in deep learning. Unfortunately, missing values are common when preparing large-scale spatiotemporal databases. Most existing imputation methods primarily focus on exploring the spatiotemporal correlations of single-source data; however, high missing rates in single-source data result in sparse distributions. Furthermore, existing methods typically focus on shallow correlations at a single scale, limiting the ability of imputation models to effectively leverage multi-scale spatial features. To tackle these challenges, we propose a multivariate dependency-aware spatiotemporal imputation model, named ST-Imputer. Specifically, we introduce multi-source context data to provide sufficient correlation features for target data ( i.e ., data that needs imputation), alleviating the issue of insufficient available features caused by high missing rates in single-source data. By applying a multi-variate spatiotemporal dependency extraction module, ST-Imputer captures potential associations between different spatial scales. Subsequently, the noise prediction module utilizes the learned dual-view features to formulate the spatiotemporal transmission module, thereby reducing weight errors caused by excessive noise. Finally, physical constraints are applied to prevent unrealistic predictions. Extensive experiments on three large-scale datasets demonstrate the significant superiority of ST-Imputer, achieving up to a 13.07% improvement in RMSE. The code of our model is available at https://github.com/Lion1a/ST-Imputer .&lt;/p&gt;</content:encoded></item><item><title>ChatAssistDesign: A Language-Interactive Framework for Iterative Vector Floorplan Generation via Conditional Diffusion</title><link>https://doi.org/10.1016/j.inffus.2025.104091</link><guid>10.1016/j.inffus.2025.104091</guid><pubDate>Sat, 20 Dec 2025 23:23:06 +0000</pubDate><dc:creator>Luping Li</dc:creator><dc:creator>Xing Su</dc:creator><dc:creator>Han Lin</dc:creator><dc:creator>Haoying Han</dc:creator><dc:creator>Chao Fan</dc:creator><dc:creator>Zhao Zhang</dc:creator><dc:creator>Hongzhe Yue</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104091</prism:doi><description>Architectural design, a complex optimization process requiring iterative revisions by skilled architects, increasingly leverages computational tools. While deep generative models show promise in automating floorplan generation, two key limitations persist: (1) reliance on domain expertise, creating high technical barriers for non-experts, and (2) lack of iterative refinement capabilities, limiting post-generation adjustments. To address these challenges, we propose ChatAssistDesign, an interactive text-driven framework combining (1) Floorplan Designer, a large language model (LLM) agent guiding users through design workflows, and (2) ConDiffPlan, a vector-based conditional diffusion model for layout generation. Extensive experimental results demonstrate that our framework achieves significant improvements over state-of-the-art methods in terms of layout diversity, visual realism, text-to-layout alignment accuracy, and crucially, the ability to support iterative refinement while maintaining high robustness against constraint conflicts. By abstracting design complexity from user skill and enabling dynamic post hoc edits, our approach reduces entry barriers and improves integration with downstream tasks.
Published: 2025-12-20T23:23:06+00:00
Venue: Information Fusion
Score: 0.389 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Luping Li; Xing Su; Han Lin; Haoying Han; Chao Fan; Zhao Zhang; Hongzhe Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104091"&gt;10.1016/j.inffus.2025.104091&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.389 (ignore)&lt;/p&gt;
&lt;p&gt;Architectural design, a complex optimization process requiring iterative revisions by skilled architects, increasingly leverages computational tools. While deep generative models show promise in automating floorplan generation, two key limitations persist: (1) reliance on domain expertise, creating high technical barriers for non-experts, and (2) lack of iterative refinement capabilities, limiting post-generation adjustments. To address these challenges, we propose ChatAssistDesign, an interactive text-driven framework combining (1) Floorplan Designer, a large language model (LLM) agent guiding users through design workflows, and (2) ConDiffPlan, a vector-based conditional diffusion model for layout generation. Extensive experimental results demonstrate that our framework achieves significant improvements over state-of-the-art methods in terms of layout diversity, visual realism, text-to-layout alignment accuracy, and crucially, the ability to support iterative refinement while maintaining high robustness against constraint conflicts. By abstracting design complexity from user skill and enabling dynamic post hoc edits, our approach reduces entry barriers and improves integration with downstream tasks.&lt;/p&gt;</content:encoded></item><item><title>Beyond Homophily: Adaptive Cross-Frequency Convolution for Hypergraph Learning</title><link>https://doi.org/10.1016/j.knosys.2025.115064</link><guid>10.1016/j.knosys.2025.115064</guid><pubDate>Sat, 20 Dec 2025 23:21:03 +0000</pubDate><dc:creator>Changqin Huang</dc:creator><dc:creator>Jialin Sun</dc:creator><dc:creator>Liangliang Zha</dc:creator><dc:creator>Yi Wang</dc:creator><dc:creator>Xiaodi Huang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115064</prism:doi><description>Hypergraph Neural Networks (HNNs) have attracted considerable attention owing to their ability to model higher-order correlations, achieving success in multi-media data analytics, encompassing various domains such as multimedia fusion and visual recognition. However, existing HNN architectures struggle with hypergraphs that have low homophily ratios, where hyperedges connect semantically dissimilar nodes, resulting in performance degradation owing to heterophily interference in local message passing. To overcome this limitation, we propose an Adaptive Cross-Frequency HNN(ACF-HNN). Our proposed framework introduces three key innovations: (1) multi-band frequency filtering, which effectively captures both local and non-local hypergraph signal characteristics across spectral domains; (2) a cross-frequency fusion method, that adaptively balances multi-frequency signals while mitigating heterophily interference; and (3) a computationally efficient spatial implementation, which avoids explicit spectral decomposition, enhancing scalability. Extensive evaluations on nine benchmark datasets, covering both homophilic and heterophilic hypergraphs, demonstrate that ACF-HNN achieves state-of-the-art performance while being at least 4.3× faster than competing methods on large-scale graphs. In addition, cross-domain validation on visual recognition tasks highlights the effectiveness of ACF-HNN in complex multimodal scenarios. Our code is available at https://github.com/goll123123/ACF-HNN .
Published: 2025-12-20T23:21:03+00:00
Venue: Knowledge-Based Systems
Score: 0.356 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changqin Huang; Jialin Sun; Liangliang Zha; Yi Wang; Xiaodi Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115064"&gt;10.1016/j.knosys.2025.115064&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.356 (ignore)&lt;/p&gt;
&lt;p&gt;Hypergraph Neural Networks (HNNs) have attracted considerable attention owing to their ability to model higher-order correlations, achieving success in multi-media data analytics, encompassing various domains such as multimedia fusion and visual recognition. However, existing HNN architectures struggle with hypergraphs that have low homophily ratios, where hyperedges connect semantically dissimilar nodes, resulting in performance degradation owing to heterophily interference in local message passing. To overcome this limitation, we propose an Adaptive Cross-Frequency HNN(ACF-HNN). Our proposed framework introduces three key innovations: (1) multi-band frequency filtering, which effectively captures both local and non-local hypergraph signal characteristics across spectral domains; (2) a cross-frequency fusion method, that adaptively balances multi-frequency signals while mitigating heterophily interference; and (3) a computationally efficient spatial implementation, which avoids explicit spectral decomposition, enhancing scalability. Extensive evaluations on nine benchmark datasets, covering both homophilic and heterophilic hypergraphs, demonstrate that ACF-HNN achieves state-of-the-art performance while being at least 4.3× faster than competing methods on large-scale graphs. In addition, cross-domain validation on visual recognition tasks highlights the effectiveness of ACF-HNN in complex multimodal scenarios. Our code is available at https://github.com/goll123123/ACF-HNN .&lt;/p&gt;</content:encoded></item><item><title>DVAE: A Dynamic Variational Autoencoder for Structured Causal Discovery with Application in Biomedical Time Series</title><link>https://doi.org/10.1016/j.knosys.2025.115154</link><guid>10.1016/j.knosys.2025.115154</guid><pubDate>Sat, 20 Dec 2025 23:20:52 +0000</pubDate><dc:creator>Khashayar Bayati</dc:creator><dc:creator>Soosan Beheshti</dc:creator><dc:creator>Karthikeyan Umapathy</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115154</prism:doi><description>Causal discovery in time-series data is critical for analyzing dynamic systems across neuroscience, economics, and biomedical signal processing. Traditional methods, such as Vector Auto-regression (VAR) and constraint-based approaches, struggle with high-dimensional dependencies, nonlinear relationships, and non-stationary dynamics. Deep learning-based models, including cMLP, cLSTM, and VAE-based approaches, aim to address these challenges but suffer from instability, over-pruning, and reliance on sparsity constraints. While cMLP provides lag-specific causal inference, its accuracy is limited, and other methods fail to explicitly capture lag-wise dependencies. This paper introduces DVAE-GC, a structured deep learning framework integrating dynamic variational inference with lag-structured recurrent MLPs (lsrMLP) to explicitly model time-lagged causal dependencies. Unlike prior methods that infer causality via weight sparsity, DVAE-GC progressively refines causal estimation, leveraging a bidirectional recurrent encoder and structured decoder. Additionally, Noise Invalidation Soft Thresholding (NIST) eliminates spurious connections, enhancing interpretability and robustness.
Empirically, DVAE-GC outperforms the best baseline (CUTS) on VAR(9) by +18.3 absolute F1 points averaged over multiple noise levels, and on NetSim fMRI-20 by +8.1 absolute F1 points averaged over sequence multiple lengths; in simulated atrial rotor detection, it improves Rotational Activity Estimation Precision (RAEP) by +22.4 % over the best alternative (VAR). These are absolute-point gains, and also precision, recall, and false discovery rate (FDR) has been reported. Although evaluated in biomedical simulations, DVAE-GC applies broadly to time-series domains, including neuroscience, climate science, and financial modeling.
Published: 2025-12-20T23:20:52+00:00
Venue: Knowledge-Based Systems
Score: 0.353 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Khashayar Bayati; Soosan Beheshti; Karthikeyan Umapathy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115154"&gt;10.1016/j.knosys.2025.115154&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.353 (ignore)&lt;/p&gt;
&lt;p&gt;Causal discovery in time-series data is critical for analyzing dynamic systems across neuroscience, economics, and biomedical signal processing. Traditional methods, such as Vector Auto-regression (VAR) and constraint-based approaches, struggle with high-dimensional dependencies, nonlinear relationships, and non-stationary dynamics. Deep learning-based models, including cMLP, cLSTM, and VAE-based approaches, aim to address these challenges but suffer from instability, over-pruning, and reliance on sparsity constraints. While cMLP provides lag-specific causal inference, its accuracy is limited, and other methods fail to explicitly capture lag-wise dependencies. This paper introduces DVAE-GC, a structured deep learning framework integrating dynamic variational inference with lag-structured recurrent MLPs (lsrMLP) to explicitly model time-lagged causal dependencies. Unlike prior methods that infer causality via weight sparsity, DVAE-GC progressively refines causal estimation, leveraging a bidirectional recurrent encoder and structured decoder. Additionally, Noise Invalidation Soft Thresholding (NIST) eliminates spurious connections, enhancing interpretability and robustness.
Empirically, DVAE-GC outperforms the best baseline (CUTS) on VAR(9) by +18.3 absolute F1 points averaged over multiple noise levels, and on NetSim fMRI-20 by +8.1 absolute F1 points averaged over sequence multiple lengths; in simulated atrial rotor detection, it improves Rotational Activity Estimation Precision (RAEP) by +22.4 % over the best alternative (VAR). These are absolute-point gains, and also precision, recall, and false discovery rate (FDR) has been reported. Although evaluated in biomedical simulations, DVAE-GC applies broadly to time-series domains, including neuroscience, climate science, and financial modeling.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Graph Neural Networks through Universal Self-Knowledge Distillation</title><link>https://doi.org/10.1016/j.neunet.2025.108505</link><guid>10.1016/j.neunet.2025.108505</guid><pubDate>Sat, 20 Dec 2025 23:20:22 +0000</pubDate><dc:creator>Zheng ZhongZhu</dc:creator><dc:creator>Pei Zhou</dc:creator><dc:creator>Renyuan Liu</dc:creator><dc:creator>Jiangping Zhu</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108505</prism:doi><description>Compared to graph distillation, graph self-distillation has gained increasing attention due to its lower memory and time requirements. In recent years, various methods have explored the use of handcrafted soft labels to improve student performance in less time than graph knowledge distillation (KD). These approaches generally acquire labels through auxiliary branches or contrastive learning. While they are faster than graph knowledge distillation, they still involve considerable overhead when compared to directly training models. To address the limitations of these methods, we propose a general and effective soft label acquisition method called Universal Graph Self-Knowledge Distillation(UGKD). Unlike traditional knowledge distillation, UGKD enables the model to distill knowledge from its own intermediate outputs by using the student’s target logit as soft target labels and generates soft non-target labels based on the ranks of intermediate features according to Zipf’s law. The UGKD method is the first graph self-knowledge distillation method that works well with both MLP and GNN models with very little extra time and memory usage, leading to state-of-the-art results. The code is available at https://www.github.com/2251821381/UGKD
Published: 2025-12-20T23:20:22+00:00
Venue: Neural Networks
Score: 0.336 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zheng ZhongZhu; Pei Zhou; Renyuan Liu; Jiangping Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108505"&gt;10.1016/j.neunet.2025.108505&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.336 (ignore)&lt;/p&gt;
&lt;p&gt;Compared to graph distillation, graph self-distillation has gained increasing attention due to its lower memory and time requirements. In recent years, various methods have explored the use of handcrafted soft labels to improve student performance in less time than graph knowledge distillation (KD). These approaches generally acquire labels through auxiliary branches or contrastive learning. While they are faster than graph knowledge distillation, they still involve considerable overhead when compared to directly training models. To address the limitations of these methods, we propose a general and effective soft label acquisition method called Universal Graph Self-Knowledge Distillation(UGKD). Unlike traditional knowledge distillation, UGKD enables the model to distill knowledge from its own intermediate outputs by using the student’s target logit as soft target labels and generates soft non-target labels based on the ranks of intermediate features according to Zipf’s law. The UGKD method is the first graph self-knowledge distillation method that works well with both MLP and GNN models with very little extra time and memory usage, leading to state-of-the-art results. The code is available at https://www.github.com/2251821381/UGKD&lt;/p&gt;</content:encoded></item><item><title>Causal Representation Learning via Graph Attention Mechanism with Aggregating Causal Information</title><link>https://doi.org/10.1016/j.knosys.2025.115150</link><guid>10.1016/j.knosys.2025.115150</guid><pubDate>Sat, 20 Dec 2025 23:21:13 +0000</pubDate><dc:creator>Dianlong You</dc:creator><dc:creator>Jiawei Shen</dc:creator><dc:creator>Zexuan Li</dc:creator><dc:creator>Chuan Lu</dc:creator><dc:creator>Zhen Chen</dc:creator><dc:creator>Xindong Wu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115150</prism:doi><description>Learning causal representations from unstructured data is a challenging problem due to the sophisticated feature relationships, confounding factors, and nonlinear distributions, resulting in ineffective responses to existing methods. To address this issue, this paper proposes a new Causal Representation Learning model using Triplet network and Graph Attention mechanism (CRL TGA ) with aggregating causal information. CRL TGA has a three-fold main idea: 1) injecting graph attention network (GAT) into a structural causal model (SCM) for aggregating causal information from context nodes and exploiting gradient-based strategies to update GAT continuously, and then obtaining a causal structure matrix; 2) using triplet loss to reduce the distance between similar samples in latent space distribution for obtaining more effective causal representations, and 3) jointly training a variational autoencoder (VAE) and generative adversarial network (GAN) for facilitating the decoder to generate more realistic intervention samples. Furthermore, we develop a synthetic dataset(C3dtree) with complex causal relationships to simulate real-world scenarios. To evaluate performance, we compare CRL TGA with its rivals on synthetic and real-world datasets by metric indicators and intervention experiments. Additionally, we conduct ablation experiments and exploit downstream tasks to evaluate the effectiveness of causal representations and robustness in handling distributional bias. Code and C3dtree dataset are released at https://github.com/youdianlong/CRLTGA.git .
Published: 2025-12-20T23:21:13+00:00
Venue: Knowledge-Based Systems
Score: 0.324 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dianlong You; Jiawei Shen; Zexuan Li; Chuan Lu; Zhen Chen; Xindong Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115150"&gt;10.1016/j.knosys.2025.115150&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.324 (ignore)&lt;/p&gt;
&lt;p&gt;Learning causal representations from unstructured data is a challenging problem due to the sophisticated feature relationships, confounding factors, and nonlinear distributions, resulting in ineffective responses to existing methods. To address this issue, this paper proposes a new Causal Representation Learning model using Triplet network and Graph Attention mechanism (CRL TGA ) with aggregating causal information. CRL TGA has a three-fold main idea: 1) injecting graph attention network (GAT) into a structural causal model (SCM) for aggregating causal information from context nodes and exploiting gradient-based strategies to update GAT continuously, and then obtaining a causal structure matrix; 2) using triplet loss to reduce the distance between similar samples in latent space distribution for obtaining more effective causal representations, and 3) jointly training a variational autoencoder (VAE) and generative adversarial network (GAN) for facilitating the decoder to generate more realistic intervention samples. Furthermore, we develop a synthetic dataset(C3dtree) with complex causal relationships to simulate real-world scenarios. To evaluate performance, we compare CRL TGA with its rivals on synthetic and real-world datasets by metric indicators and intervention experiments. Additionally, we conduct ablation experiments and exploit downstream tasks to evaluate the effectiveness of causal representations and robustness in handling distributional bias. Code and C3dtree dataset are released at https://github.com/youdianlong/CRLTGA.git .&lt;/p&gt;</content:encoded></item><item><title>A2R: A Hybrid Activation-Attention Framework for Enhancing Large Language Model Reliability</title><link>https://doi.org/10.1016/j.eswa.2025.130922</link><guid>10.1016/j.eswa.2025.130922</guid><pubDate>Sat, 20 Dec 2025 23:21:10 +0000</pubDate><dc:creator>Xuran Li</dc:creator><dc:creator>Jingyi Wang</dc:creator><dc:creator>Xiaohan Yuan</dc:creator><dc:creator>Wenhai Wang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130922</prism:doi><description>Large language models (LLMs) have demonstrated impressive capabilities in NLP tasks but remain prone to factual inaccuracies, outdated knowledge, and unsafe outputs. Addressing these reliability issues without full-scale retraining is a critical challenge. Existing model editing approaches, such as fine-tuning and hypernetwork-based methods, often suffer from unintended side effects, imprecise updates, or high computational costs. In this work, we propose A ctivation and A ttention-based model R epair (A 2 R), a novel framework for efficiently and precisely modifying LLMs without compromising overall performance. A 2 R employs a hybrid activation-attention mechanism to identify and target the most relevant model layers for intervention, ensuring localized and interpretable updates. Additionally, we introduce a structured optimization objective that balances repair accuracy, knowledge retention, and behavioral consistency, preventing overcorrection or unintended degradation. Extensive experiments on multiple LLMs demonstrate that A 2 R achieves remarkable repair success rates, effectively improving factual accuracy and mitigating harmful outputs with minimal computational overhead. Our approach offers a scalable and interpretable solution for enhancing LLM reliability, paving the way for safer and more trustworthy AI deployment. Our code is available at https://github.com/RitaLi1005/Project_AAR .
Published: 2025-12-20T23:21:10+00:00
Venue: Expert Systems with Applications
Score: 0.317 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuran Li; Jingyi Wang; Xiaohan Yuan; Wenhai Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130922"&gt;10.1016/j.eswa.2025.130922&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.317 (ignore)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) have demonstrated impressive capabilities in NLP tasks but remain prone to factual inaccuracies, outdated knowledge, and unsafe outputs. Addressing these reliability issues without full-scale retraining is a critical challenge. Existing model editing approaches, such as fine-tuning and hypernetwork-based methods, often suffer from unintended side effects, imprecise updates, or high computational costs. In this work, we propose A ctivation and A ttention-based model R epair (A 2 R), a novel framework for efficiently and precisely modifying LLMs without compromising overall performance. A 2 R employs a hybrid activation-attention mechanism to identify and target the most relevant model layers for intervention, ensuring localized and interpretable updates. Additionally, we introduce a structured optimization objective that balances repair accuracy, knowledge retention, and behavioral consistency, preventing overcorrection or unintended degradation. Extensive experiments on multiple LLMs demonstrate that A 2 R achieves remarkable repair success rates, effectively improving factual accuracy and mitigating harmful outputs with minimal computational overhead. Our approach offers a scalable and interpretable solution for enhancing LLM reliability, paving the way for safer and more trustworthy AI deployment. Our code is available at https://github.com/RitaLi1005/Project_AAR .&lt;/p&gt;</content:encoded></item><item><title>L-PromptCTRL:Learnable Prompting for Contextual Residual Anomaly Detection</title><link>https://doi.org/10.1016/j.dsp.2025.105841</link><guid>10.1016/j.dsp.2025.105841</guid><pubDate>Sat, 20 Dec 2025 23:21:44 +0000</pubDate><dc:creator>Minmin Zhou</dc:creator><dc:creator>Ying Chen</dc:creator><dc:creator>Shunyuan Sun</dc:creator><prism:publicationName>Digital Signal Processing</prism:publicationName><prism:doi>10.1016/j.dsp.2025.105841</prism:doi><description>To address insufficient generalization in industrial anomaly detection caused by scarce anomalous samples, this paper proposes L-PromptCTRL, a few-shot anomaly detection method based on object-context residual learning. Building upon InCTRL, the method introduces a deep composite learnable prompt mechanism that dynamically injects learnable prompt vectors at different text encoder layers, overcoming fixed template limitations. An enhanced multi-level feature fusion strategy utilizes multi-scale visual features through improved patch-level residual learning and hierarchical feature pyramids. A contrast-guided residual learning mechanism explicitly maximizes differences between normal and abnormal text representations. Experiments on MVTec AD, VisA, ELPV, and AITEX datasets demonstrate that L-PromptCTRL significantly outperforms existing methods in few-shot settings, achieving effective cross-domain anomaly detection with only a few normal samples.
Published: 2025-12-20T23:21:44+00:00
Venue: Digital Signal Processing
Score: 0.316 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minmin Zhou; Ying Chen; Shunyuan Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Digital Signal Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.dsp.2025.105841"&gt;10.1016/j.dsp.2025.105841&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.316 (ignore)&lt;/p&gt;
&lt;p&gt;To address insufficient generalization in industrial anomaly detection caused by scarce anomalous samples, this paper proposes L-PromptCTRL, a few-shot anomaly detection method based on object-context residual learning. Building upon InCTRL, the method introduces a deep composite learnable prompt mechanism that dynamically injects learnable prompt vectors at different text encoder layers, overcoming fixed template limitations. An enhanced multi-level feature fusion strategy utilizes multi-scale visual features through improved patch-level residual learning and hierarchical feature pyramids. A contrast-guided residual learning mechanism explicitly maximizes differences between normal and abnormal text representations. Experiments on MVTec AD, VisA, ELPV, and AITEX datasets demonstrate that L-PromptCTRL significantly outperforms existing methods in few-shot settings, achieving effective cross-domain anomaly detection with only a few normal samples.&lt;/p&gt;</content:encoded></item><item><title>In-Context Learning Enhanced by Multi-perspective Sequential Retrieval and Predictive Feedback for Few-Shot Aspect-Based Sentiment Analysis</title><link>https://doi.org/10.1016/j.eswa.2025.130869</link><guid>10.1016/j.eswa.2025.130869</guid><pubDate>Sat, 20 Dec 2025 23:21:12 +0000</pubDate><dc:creator>Jiasen Gao</dc:creator><dc:creator>Xiaoliang Chen</dc:creator><dc:creator>Duoqian Miao</dc:creator><dc:creator>Hongyun Zhang</dc:creator><dc:creator>Xiaolin Qin</dc:creator><dc:creator>Shangyi Du</dc:creator><dc:creator>Peng Lu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130869</prism:doi><description>Aspect-based sentiment analysis (ABSA) aims to extract fine-grained opinions from the text by discerning sentiments toward specific aspects. Although large language models (LLMs) perform well in in-context learning (ICL), current ICL methodologies typically retrieve semantically similar but structurally redundant examples, failing to capture syntactic and aspect-level cues critical for ABSA. To overcome these limitations, we report Multi-perspective Sequential retrieval with Predictive Feedback (MSPF), a few-shot learning framework that enhances ICL through MSPF, which integrates three complementary perspectives: overall semantic, syntactic relevance, and aspect sentiment alignment. Evaluated on four benchmark datasets (Laptop14, Restaurant14, Books, and Clothing), MSPF achieved F1 scores of 67.03% (Laptop14), 73.51% (Restaurant14), 76.07% (Books), and 81.96% (Clothing), outperforming standard ICL by +7.06%, +5.60%, +25.61%, and +18.38%, respectively. These results validated the efficacy of MSPF in improving LLM reasoning for fine-grained sentiment tasks with limited annotations.
Published: 2025-12-20T23:21:12+00:00
Venue: Expert Systems with Applications
Score: 0.311 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiasen Gao; Xiaoliang Chen; Duoqian Miao; Hongyun Zhang; Xiaolin Qin; Shangyi Du; Peng Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130869"&gt;10.1016/j.eswa.2025.130869&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.311 (ignore)&lt;/p&gt;
&lt;p&gt;Aspect-based sentiment analysis (ABSA) aims to extract fine-grained opinions from the text by discerning sentiments toward specific aspects. Although large language models (LLMs) perform well in in-context learning (ICL), current ICL methodologies typically retrieve semantically similar but structurally redundant examples, failing to capture syntactic and aspect-level cues critical for ABSA. To overcome these limitations, we report Multi-perspective Sequential retrieval with Predictive Feedback (MSPF), a few-shot learning framework that enhances ICL through MSPF, which integrates three complementary perspectives: overall semantic, syntactic relevance, and aspect sentiment alignment. Evaluated on four benchmark datasets (Laptop14, Restaurant14, Books, and Clothing), MSPF achieved F1 scores of 67.03% (Laptop14), 73.51% (Restaurant14), 76.07% (Books), and 81.96% (Clothing), outperforming standard ICL by +7.06%, +5.60%, +25.61%, and +18.38%, respectively. These results validated the efficacy of MSPF in improving LLM reasoning for fine-grained sentiment tasks with limited annotations.&lt;/p&gt;</content:encoded></item><item><title>RICA: Re-Ranking with Intra-Modal and Cross-Modal Alignment for Text-Based Person Search</title><link>https://doi.org/10.1016/j.eswa.2025.130931</link><guid>10.1016/j.eswa.2025.130931</guid><pubDate>Sat, 20 Dec 2025 23:20:56 +0000</pubDate><dc:creator>Yu Bai</dc:creator><dc:creator>Wentao Ma</dc:creator><dc:creator>Shan Zhao</dc:creator><dc:creator>Tianwei Yan</dc:creator><dc:creator>Shezheng Song</dc:creator><dc:creator>Chengyu Wang</dc:creator><dc:creator>Qian Wan</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130931</prism:doi><description>Text-based person search aims to retrieve pedestrian images using textual descriptions, a challenging task due to the semantic gap between visual and textual modalities. Existing methods focus on bridging this gap through cross-modal feature alignment but often overlook intra-modal representation consistency, leading to suboptimal embedding spaces. Moreover, balancing retrieval accuracy with computational efficiency also remains a key issue. To address these, we propose RICA, a re-ranking-based framework with enhanced cross-modal alignment. At its core is an Alignment Triplet Loss that enforces semantic consistency across and within modalities for a more robust feature space. We also introduce a lightweight local feature alignment strategy to capture fine-grained details without heavy computational overhead. Additionally, we propose a two-stage re-ranking inference pipeline that uses global features for efficient initial retrieval, followed by local similarity re-ranking of top candidates. Experiments show RICA achieves competitive or state-of-the-art performance, including 74.77% Rank-1 on CUHK-PEDES.
Published: 2025-12-20T23:20:56+00:00
Venue: Expert Systems with Applications
Score: 0.297 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Bai; Wentao Ma; Shan Zhao; Tianwei Yan; Shezheng Song; Chengyu Wang; Qian Wan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130931"&gt;10.1016/j.eswa.2025.130931&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.297 (ignore)&lt;/p&gt;
&lt;p&gt;Text-based person search aims to retrieve pedestrian images using textual descriptions, a challenging task due to the semantic gap between visual and textual modalities. Existing methods focus on bridging this gap through cross-modal feature alignment but often overlook intra-modal representation consistency, leading to suboptimal embedding spaces. Moreover, balancing retrieval accuracy with computational efficiency also remains a key issue. To address these, we propose RICA, a re-ranking-based framework with enhanced cross-modal alignment. At its core is an Alignment Triplet Loss that enforces semantic consistency across and within modalities for a more robust feature space. We also introduce a lightweight local feature alignment strategy to capture fine-grained details without heavy computational overhead. Additionally, we propose a two-stage re-ranking inference pipeline that uses global features for efficient initial retrieval, followed by local similarity re-ranking of top candidates. Experiments show RICA achieves competitive or state-of-the-art performance, including 74.77% Rank-1 on CUHK-PEDES.&lt;/p&gt;</content:encoded></item></channel></rss>